神经网络(Neural Network)
========================================

神经网络的结构
--------------
一张图解释神经网络的结构:

.. image:: ../images/nn_1.png
    :width: 800px
    :align: center

神经网络可以看作是逻辑回归的一个堆叠形式.逻辑回归本身就可以看作是一个单一的神经元,而神经网络是讲多个神经元组合到一起,多个神经元构成一个层,一个神经网络可以由多个层构成,每一层的输入依次是上一层的输出。每次一轮的向前传播,得到的值与目标值构成一个损失函数,然后用梯度下降法进行反向传播,来修正参数.


要点
--------------
* 激活函数

.. image:: ../images/nn_2.png
    :width: 800px
    :align: center

上图是常见的几种激活函数.tanh其实是sigmoid的一个平移,好处是去中心化,但是两者共有的缺点是,当z特别大或特别小时,其导数接近0,会拖慢梯度下降的计算速度.ReLU是最常用的激活函数,因为它不存在导数接近0的情况,所以计算速度会比较快.它的另一个版本叫Leaky ReLU,但是相对来说用的并不是那么多.

* Dropout正则化

.. image:: ../images/nn_3.png
    :width: 800px
    :align: center

Dropout正则化就是随机的去掉隐藏层的一些神经元.之所以会起到正则化的原因是,神经元是被随机去掉的,这样可以防止整个网络对某一个神经元,或者某一个特征的依赖,从而提升泛化能力.


* 随机初始化

神经网络的参数初始值如果都是0,或者每个隐藏层的初始参数值都一样的话,会使得每个神经元都在进行相同的计算,这样就失去了多个神经元存在的意义.所以对于参数会进行随机初始化操作,并且初始值不大不小(如1.01),以防止梯度消失和梯度爆炸.
