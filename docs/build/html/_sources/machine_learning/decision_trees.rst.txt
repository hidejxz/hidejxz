决策树
========================================
基本流程
------------
决策树的学习是一个递归的,选择最优特征和进行最优分割的过程,使得被分割的样本子集中,目标值尽可能的最纯。决策树相当于一系列条件规则模型(if-then),比较容易解读,因此应用也比较广泛。


特征划分
------------

1. C3 - 信息增益
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

熵的定义:

.. math::
    Ent(p) = -\sum_{i=1}^np_i\log p_i

当变量只有两个值时,熵的曲线如下图:

.. image:: ../images/dt_entropy.png
    :width: 300px
    :align: center

当 :math:`p=0` 或 :math:`p=1` 时,变量最纯,此时熵为0;当 :math:`p=0.5` 时,变量不确定性最大,此时熵也最大,为1;

假设离散属性 :math:`a` 有 :math:`V` 个可能的取值, 用它来划分样本集 :math:`D` ,会产生 :math:`V` 个分支结点,则信息增益可以表示为:

.. math::
    Gain(D,a) = Ent(D) - \sum_{i=1}^V\frac{|D^{v}|}{|D|}Ent(D^v)

比较各属性的信息增益,选择信息增益最大的特征进行划分。划分完之后,对产生的 :math:`V` 个子集各自继续划分,直到无法划分为止。

2. C4.5 - 信息增益比 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

C4.5相比与C3,采用的是信息增益比的方法,这样做会让划分更变偏向于取值数目较少的属性:

.. math:: 
    Gain\_ratio(D,a) = \frac{Gain(D,a)}{Ent_a(D)}

另外,C5.0也是通过信息增益率来划分结点的,在准确率,性能方面都会优于C4.5。

3. Gini指数 - CART分类
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Gini指数的定义:

.. math::
    Gini(p) = \sum_{i=1}^np_i(1-p_i) = 1-\sum_{i=1}^np_i^2

二分类问题中基尼指数,1/2熵和分类误差率的关系如图:

.. image:: ../images/dt_gini.png
    :width: 300px
    :align: center

在数据集中的Gini指数则定义为

.. math::
    Gini(D,a) = \sum_{v=1}^V\frac{|D^v|}{|D|}Gini(D^v)

选择Gini指数最小的特征进行划分。

4. 误差平方和 - CART回归
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

CART回归会遍历每个特征的每个取值,找到一个最佳切分点,使得每个子集的误差平方和最小,而其平均数就是这个子集的输出值。


剪枝
------------
剪枝的目的是防止模型过拟合。整体的损失函数可以总结为:

.. math::
    C_{\alpha}(T) = C(T) + \alpha|T|

:math:`C(T)` 为模型的预测误差(熵或者Gini指数),:math:`\|T\|` 为叶结点个数,表示模型的复杂程度。

剪枝分为前剪枝和后剪枝。前剪枝比较容易造成欠拟合,一般用后剪枝比较多。


优缺点
------------
* 优点

1.可解释性强,能可视化展现;

2.只需要较少的数据预处理,对缺失值不敏感。

* 缺点

1.决策树的增长采用的是局部最优的策略,因此不太稳定,需要用融合的方式来优化;

2.特征空间的划分都是垂直于维度的,不能解决异或问题或者多元问题。

其他补充
------------

1. CART是二叉树,个人理解1是为了尽可能避免局部最优的问题,2是防止过拟合;
2. 对于缺失值的处理,首先会根据该特征的非缺失数据集进行划分,然后再把缺失值带有权重的划分到子集中,权重由该取值在样本中的数量占比决定,即哪种取值多,权重概率就大,缺失样本被分到该类的可能性就越大;
3. 亦有多变量决策树(斜决策树)可以一定程度地解决变量划分只能平行于维度的问题。此时不再是寻找单一的特征划分点,而是去寻找一个合适的线性分类器。
