序列模型(RNN)
========================================

序列模型(Recurrent Neural Networks)的应用场景有语音识别,自然语言处理等,所训练的对象一般都是序列数据,例如音频文件,文本文件等等.序列数据的特点是,其内部数据与数据之间存在着一定的先后顺序关系.

结构
----------------

序列模型的常见结构如下图.样本与样本之间存在着一定的序列关系,每一个神经元的输入不只有对应的样本输入,也可以受到之前或者之后(BRNN)样本的影响.

.. image:: ../images/rnn_1.png
    :width: 800px
    :align: center

对于一些长序列数据,经常会遇到梯度消失的问题,导致远距离的数据信息无法传递到后面.解决这个问题通常会用LSTM(Long Short Term Memory)和GRU(Gated Recurrent Unit).其主要的原理是结合激活函数,构造信息传递的阀门来控制何时需要用到该信息数据.LSTM采用了"忘却门"和"记忆门",而GRU是LSTM的改进版,将两扇门合并成了一扇"更新门",更加简洁有效.

Word Embedding
-----------------
最原始的词汇表征方法是用one-hot,但是这种方法无法反映出词与词之间的联系.有的词汇意思相近,但是其距离并没有相应的正比关系.一个可行有效的方法就是采用Word Embadding,即用特征向量来表示每个词汇,形成一个带有特征的词汇矩阵.

word2vec是一种学习词汇嵌入的有效方法.一般分为两种:CBOW是输入上下文,输出目标词汇;Skip-Gram是输入目标词汇,输出上下文.模型用神经网络进行训练,目的不是得到输出值,而是的到隐藏层中的词汇特征矩阵.

.. image:: ../images/rnn_2.jpeg
    :width: 800px
    :align: center

对于优化计算,常用的有Hierarchical Softmax和Negative Sampling:Hierarchical Softmax是一种对输出层进行优化的策略,输出层从原始模型的利用softmax计算概率值改为了利用Huffman树计算概率值;Negative Sampling的思想是,把语料中的一个词串的中心词替换为别的词，构造语料中不存在的词串作为负样本,在这种策略下,优化目标变为最大化正样本的概率，同时最小化负样本的概率。